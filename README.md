<div align="center">
  <img src="https://readme-typing-svg.herokuapp.com?color=%230070FF&size=32&center=true&vCenter=true&width=600&height=50&lines=Adventure+Works+Project&font=Arial+Black" alt="Headline" />
</div>




 
 <div align=center>
         <a href="https://imgbb.com/"><img src="https://i.ibb.co/b5Ry5S8h/Github-Image.gif" alt="Github-Image" border="0"></a>
  </div>


<h1 align="center">🌟  ABOUT THIS REPOSITORY 🌟 </h1>

<p align="center" style="display: flex; justify-content: center; gap: 20px;">
  <a href="https://ibb.co/RpyzfdcF">
    <img src="https://i.ibb.co/Z6mW57xj/Dashboard.jpg" alt="Dashboard" width="400" />
  </a>
  <a href="https://ibb.co/qLfNMHxQ">
    <img src="https://i.ibb.co/8nprDyXZ/page-2.jpg" alt="Page-2" width="400" />
  </a>
</p>

💡 **_Key things Learn_**


      1. Data Ingestion: 
          ✅ Extract customer and sales data from an on-premises SQL database.
          ✅ Load the data into Azure Data Lake Storage (ADLS) using Azure Data Factory (ADF).

      2. Data Transformation:
          ✅ Use Azure Databricks to clean and transform the data.
          ✅ Organize the data into Bronze, Silver, and Gold layers for raw, cleansed, and aggregated data respectively.
      
      3. Data Loading and Reporting:
          ✅ Load the transformed data into Azure Synapse Analytics.
          ✅ Build a Power BI dashboard to visualize the data, allowing stakeholders to explore sales and demographic insights.
      
      4. Automation:
          ✅ Schedule the pipeline to run daily, ensuring that the data and reports are always up-to-date.

🔥 **_Tech Stack & Tools_**


    - Azure Data Factory (ADF): For orchestrating data movement and transformation.
    - Azure Data Lake Storage (ADLS): For storing raw and processed data.
    - Azure Databricks: For data transformation and processing.
    - Azure Synapse Analytics: For data warehousing and SQL-based analytics.
    - Power BI: For data visualization and reporting.
    - Azure Key Vault: For securely managing credentials and secrets.
    - SQL Server (On-Premises): Source of customer and sales data.

  
  🎯 **_Goals_**

    📚 Learn Data Engineering Concepts
    🧹 Data Cleaning and Preprocessing
    🔄 Data Transformation using PySpark
    📊 Getting Data-Driven Decisions

📌 **_How to Use ?_**

    
    Prerequisites
                                                                                  
                ✅ An Azure account with sufficient credits.
                ✅ Access to an on-premises SQL Server database.

    Step 1️⃣: Azure Environment Setup
    
              1. Create Resource Group: Set up a new resource group in Azure.
              2. Provision Services:
                 - Create an Azure Data Factory instance.
                 - Set up Azure Data Lake Storage with `bronze`, `silver`, and `gold` containers.
                 - Set up an Azure Databricks workspace and Synapse Analytics workspace.
                 - Configure Azure Key Vault for secret management.
    
    Step 2️⃣: Data Ingestion
    
              1. Set up SQL Server: Install SQL Server and SQL Server Management Studio (SSMS). Restore the AdventureWorks database.
              2. Ingest Data with ADF: Create pipelines in ADF to copy data from SQL Server to the `bronze` layer in ADLS.
    
    Step 3️⃣: Data Transformation
    
              1. Mount Data Lake in Databricks: Configure Databricks to access ADLS.
              2. Transform Data: Use Databricks notebooks to clean and aggregate the data, 
                 moving it from `bronze` to `silver` and then to `gold`.
    
    Step 4️⃣: Data Loading and Reporting
    
              1. Load Data into Synapse: Set up a Synapse SQL pool and load the `gold` data for analysis.
              2. Create Power BI Dashboard: Connect Power BI to Synapse and create visualizations based on business requirements.
    
    Step 5️⃣: Automation and Monitoring
    
              1. Schedule Pipelines: Use ADF to schedule the data pipelines to run daily.
              2. Monitor Pipeline Runs: Use the monitoring tools in ADF and Synapse to ensure successful pipeline execution.
    
    Step 6️⃣: Security and Governance
    
              1. Manage Access: Set up role-based access control (RBAC) using Azure Entra ID (formerly Active Directory).
    
    Step 7️⃣: End-to-End Testing
    
              1. Trigger and Test Pipelines: Insert new records into the SQL database and verify 
                 that the entire pipeline runs successfully,updating the Power BI dashboard.

📬 **_Contact & Connect_**

      👤 GitHub Profile:
      💼 Portfolio Website: Pending
      📧 Email: ushanloshitha@gmail.com

<p align="center">
  <a href="https://git.io/typing-svg">
    <img src="https://readme-typing-svg.demolab.com?font=Arial+Black&letterSpacing=8px&pause=1000&color=CCD713&background=FF306200&center=true&vCenter=true&width=435&lines=%F0%9F%9A%80+Good+Luck++!+%F0%9F%91%A8%E2%80%8D%F0%9F%92%BB%E2%9C%A8" alt="Typing SVG" />
  </a>
</p>
  
